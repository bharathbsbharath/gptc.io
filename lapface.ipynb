{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f4e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gptc1\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bf019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add666ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-02-26 13:08:18 - ⚠️ Items ['./Database/Bill gates/GA311881_Bill_Gates.jpg', './Database/Bill gates/nTGMV1Eo_400x400.jpg', './Database/Elon/WSIRZRWN6BOMBA4VHAUGK32TVA.jpg', './Database/Bill gates/cf32e15a-c5a8-4665-94d7-b56d6ff8db55_w948_r1.778_fpx54_fpy36.jpg', './Database//test.jpeg', './Database/Elon/elon-musk-rt-jt-230511_1683834880758_hpMain_1x1_608.jpg'] were added into ./Database/ just after data source ./Database//representations_vgg_face.pkl created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding representations:  33%|███▎      | 2/6 [00:05<00:11,  2.94s/it]\n"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "Exception encountered when calling layer 'conv2d_13' (type Conv2D).\n\n{{function_node __wrapped____MklNativeConv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:1093 [Op:Conv2D] name: \n\nCall arguments received by layer 'conv2d_13' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 7, 7, 512), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m res \u001b[38;5;241m=\u001b[39m DeepFace\u001b[38;5;241m.\u001b[39mfind(frame, db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Database/\u001b[39m\u001b[38;5;124m'\u001b[39m, enforce_detection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG-Face\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     11\u001b[0m     name \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\DeepFace.py:296\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind\u001b[39m(\n\u001b[0;32m    229\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    230\u001b[0m     db_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m     silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m    241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    Identify individuals in a database\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m                specified model and distance metric\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recognition\u001b[38;5;241m.\u001b[39mfind(\n\u001b[0;32m    297\u001b[0m         img_path\u001b[38;5;241m=\u001b[39mimg_path,\n\u001b[0;32m    298\u001b[0m         db_path\u001b[38;5;241m=\u001b[39mdb_path,\n\u001b[0;32m    299\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    300\u001b[0m         distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric,\n\u001b[0;32m    301\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    302\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    303\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    304\u001b[0m         expand_percentage\u001b[38;5;241m=\u001b[39mexpand_percentage,\n\u001b[0;32m    305\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m    306\u001b[0m         normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    307\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\recognition.py:134\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(img_path, db_path, model_name, distance_metric, enforce_detection, detector_backend, align, expand_percentage, threshold, normalization, silent)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newbies:\n\u001b[0;32m    130\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItems \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnewbies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were added into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m just after data source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatastore_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m created!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n\u001b[1;32m--> 134\u001b[0m     newbies_representations \u001b[38;5;241m=\u001b[39m __find_bulk_embeddings(\n\u001b[0;32m    135\u001b[0m         employees\u001b[38;5;241m=\u001b[39mnewbies,\n\u001b[0;32m    136\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    137\u001b[0m         target_size\u001b[38;5;241m=\u001b[39mtarget_size,\n\u001b[0;32m    138\u001b[0m         detector_backend\u001b[38;5;241m=\u001b[39mdetector_backend,\n\u001b[0;32m    139\u001b[0m         enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    140\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    141\u001b[0m         normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    142\u001b[0m         silent\u001b[38;5;241m=\u001b[39msilent,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     representations \u001b[38;5;241m=\u001b[39m representations \u001b[38;5;241m+\u001b[39m newbies_representations\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m oldies:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\recognition.py:370\u001b[0m, in \u001b[0;36m__find_bulk_embeddings\u001b[1;34m(employees, model_name, target_size, detector_backend, enforce_detection, align, expand_percentage, normalization, silent)\u001b[0m\n\u001b[0;32m    368\u001b[0m img_content \u001b[38;5;241m=\u001b[39m img_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    369\u001b[0m img_region \u001b[38;5;241m=\u001b[39m img_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacial_area\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 370\u001b[0m embedding_obj \u001b[38;5;241m=\u001b[39m representation\u001b[38;5;241m.\u001b[39mrepresent(\n\u001b[0;32m    371\u001b[0m     img_path\u001b[38;5;241m=\u001b[39mimg_content,\n\u001b[0;32m    372\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    373\u001b[0m     enforce_detection\u001b[38;5;241m=\u001b[39menforce_detection,\n\u001b[0;32m    374\u001b[0m     detector_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    375\u001b[0m     align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m    376\u001b[0m     normalization\u001b[38;5;241m=\u001b[39mnormalization,\n\u001b[0;32m    377\u001b[0m )\n\u001b[0;32m    379\u001b[0m img_representation \u001b[38;5;241m=\u001b[39m embedding_obj[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    381\u001b[0m instance \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\modules\\representation.py:107\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# custom normalization\u001b[39;00m\n\u001b[0;32m    105\u001b[0m img \u001b[38;5;241m=\u001b[39m preprocessing\u001b[38;5;241m.\u001b[39mnormalize_input(img\u001b[38;5;241m=\u001b[39mimg, normalization\u001b[38;5;241m=\u001b[39mnormalization)\n\u001b[1;32m--> 107\u001b[0m embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfind_embeddings(img)\n\u001b[0;32m    109\u001b[0m resp_obj \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    110\u001b[0m resp_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\deepface\\basemodels\\VGGFace.py:62\u001b[0m, in \u001b[0;36mVggFaceClient.find_embeddings\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03mfind embeddings with VGG-Face model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    embeddings (list): multi-dimensional vector\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# model.predict causes memory issue when it is called in a for loop\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# embedding = model.predict(img, verbose=0)[0].tolist()\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# having normalization layer in descriptor troubles for some gpu users (e.g. issue 957, 966)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# instead we are now calculating it with traditional way not with keras backend\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(img, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     63\u001b[0m embedding \u001b[38;5;241m=\u001b[39m verification\u001b[38;5;241m.\u001b[39ml2_normalize(embedding)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAbortedError\u001b[0m: Exception encountered when calling layer 'conv2d_13' (type Conv2D).\n\n{{function_node __wrapped____MklNativeConv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:1093 [Op:Conv2D] name: \n\nCall arguments received by layer 'conv2d_13' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 7, 7, 512), dtype=float32)"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    state, frame = cap.read()\n",
    "    if not state:\n",
    "        break\n",
    "\n",
    "    res = DeepFace.find(frame, db_path='./Database/', enforce_detection=False, model_name='VGG-Face')\n",
    "\n",
    "    if len(res) > 0 and len(res[0]['identity']) > 0:\n",
    "        name = res[0]['identity'][0].split('/')[1].split('\\\\')[1]\n",
    "        xmin = int(res[0]['source_x'][0])\n",
    "        ymin = int(res[0]['source_y'][0])\n",
    "        w = res[0]['source_w'][0]\n",
    "        h = res[0]['source_h'][0]\n",
    "\n",
    "        xmax = int(xmin + w)\n",
    "        ymax = int(ymin + h)\n",
    "\n",
    "        img = frame[ymin:ymax, xmin:xmax]\n",
    "        img = cv2.resize(img, (32,32))\n",
    "        img = img.astype('float')/255.0\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = numpy.expand_dims(img, axis=0)\n",
    "\n",
    "        liveness = model.predict(img)\n",
    "        liveness = liveness[0].argmax()\n",
    "\n",
    "        if liveness == 1:\n",
    "            cv2.rectangle(frame, (xmin,ymin),(xmax,ymax),(0,0,255),1)\n",
    "            cv2.rectangle(frame, (xmin, ymin-25),(xmax, ymin),(255, 255, 255), -1)\n",
    "            cv2.putText(frame, name, (xmin,ymin), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('attendance', frame)\n",
    "\n",
    "    c = cv2.waitKey(1)\n",
    "\n",
    "    if c == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98edcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-02-26 13:03:44 - There are 7 representations found in representations_vgg_face.pkl\n",
      "24-02-26 13:03:44 - find function lasts 3.0751261711120605 seconds\n"
     ]
    }
   ],
   "source": [
    "model = DeepFace.find(img_path='test.jpeg', db_path='./Database', enforce_detection=False, model_name='VGG-Face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e37e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Missing backslash in name part\n"
     ]
    }
   ],
   "source": [
    "if len(model) > 0 and 'identity' in model[0] and len(model[0]['identity']) > 0:\n",
    "    identity = model[0]['identity'][0]\n",
    "    parts = identity.split('/')\n",
    "    if len(parts) > 1:\n",
    "        name_parts = parts[1].split('\\\\')\n",
    "        if len(name_parts) > 1:\n",
    "            name = name_parts[1]\n",
    "            print(name)\n",
    "        else:\n",
    "            print(\"Error: Missing backslash in name part\")\n",
    "    else:\n",
    "        print(\"Error: Missing forward slash in identity\")\n",
    "else:\n",
    "    print(\"Error: No data or invalid format in model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37845b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0]['source_x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bc7d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                            identity  target_x  target_y  \\\n",
       " 0                               ./Database/test.jpeg       109        21   \n",
       " 1  ./Database\\Elon/elon-musk-rt-jt-230511_1683834...        70        81   \n",
       " 2     ./Database\\Elon/WSIRZRWN6BOMBA4VHAUGK32TVA.jpg      1499       678   \n",
       " \n",
       "    target_w  target_h  source_x  source_y  source_w  source_h  threshold  \\\n",
       " 0        79        79       109        21        79        79       0.68   \n",
       " 1       366       366       109        21        79        79       0.68   \n",
       " 2      1770      1770       109        21        79        79       0.68   \n",
       " \n",
       "    distance  \n",
       " 0  0.000000  \n",
       " 1  0.237533  \n",
       " 2  0.270936  ]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d2f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d583bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
